# Indian Startup Funding ‚Äî Automated End-to-End ETL Pipeline

An automated **data engineering pipeline** that scrapes, cleans, stores, and visualizes real-time funding data for Indian startups.  
This project demonstrates how raw web data can be transformed into **analytics-ready insights** using a **Medallion Architecture** and modern cloud-native tooling.

---

## Technical Stack & Tools

| Layer | Technology |
|-----|-----------|
| **Extraction** | Python, Playwright (Headless Chromium) |
| **Orchestration** | GitHub Actions (CI/CD, CRON scheduling) |
| **Data Warehouse** | Google BigQuery (Serverless SQL) |
| **Analytics** | Power BI (Microsoft Fabric) |
| **Security** | GitHub Secrets, GCP Service Accounts |

---

## Key Engineering Concepts Applied

### Medallion Architecture
- **Bronze**: Raw scraped web data
- **Silver**: Cleaned & standardized staging layer
- **Gold**: Historical fact table for analytics

### Idempotent UPSERT (MERGE)
- Ensures **data integrity**
- Updates existing records and inserts new ones
- Composite key used: (Company, Country, Industry, Last Funding Date)

### ‚òÅÔ∏è Cloud-to-Cloud Integration
- Direct integration between **Google BigQuery** and **Microsoft PowerBI**
- No local gateway required
- Fully automated using Scheduled Refresh

### ü§ñ Headless Automation
- Browser-based scraping using **Playwright**
- Runs in a Linux environment inside GitHub Actions
- Handles dynamic & asynchronous web content

---

## üìÇ Project Architecture & Data Flow

### 1Ô∏è‚É£ Extraction & Bronze Layer
- Python script uses **Playwright** to scrape startup funding data
- Handles JavaScript-rendered content missed by traditional scrapers
- Fully automated via GitHub Actions

**Schedule (CRON):**
30 18 * * 1
Runs every **Monday at 6:30 PM UTC**


### 2Ô∏è‚É£ Transformation ‚Äî Silver Medallion
Data is cleaned and standardized directly inside **BigQuery** using SQL (ELT approach).

**Key Transformations:**
- Uppercasing company and country names
- Parsing strings into `DATE` formats
- Splitting and normalizing industry categories

**Output Table:**
stg_clean_latest

## 3Ô∏è‚É£ Historical Persistence ‚Äî Gold Medallion
A **True UPSERT strategy** maintains historical funding records.

**Why?**
- Startups raise multiple funding rounds
- Company name alone is not a reliable key

**Matching Logic:**
- Company
- Industry
- Funding Date

**Behavior:**
- Existing round updated ‚Üí record refreshed
- New funding round ‚Üí new row inserted

**Final Table:**
fct_clean_history


---

## üìä Data Modeling

### Logical Data Model
The diagram below represents the schema and enforced data types for the `fct_clean_history` table.

üìå _Insert BigQuery schema screenshot here_

---

### Dashboard Preview
A **Power BI dashboard** built on the Gold layer provides:
- Funding trends over time
- Industry-wise investment distribution
- Geographic insights

üîÑ Automatically refreshes every **Tuesday morning** after the pipeline run.

üìå _Insert Power BI dashboard screenshot here_

---

## üõ†Ô∏è Installation & Local Setup

### 1Ô∏è‚É£ Clone the Repository
```bash
git clone https://github.com/your-username/repo-name.git
cd repo-name
```

### 2Ô∏è‚É£ Install Dependencies
```bash
pip install -r requirements.txt
playwright install chromium --with-deps
```

### 3Ô∏è‚É£ Configure Credentials

1. Create a Google Cloud Service Account
2. Download the JSON key
3. Save it locally as:
```vbnet
bigquery-key.json
```
‚ö†Ô∏è This file is ignored via .gitignore

### 4Ô∏è‚É£ Run the Script Locally
```bash
python web_scraping.py
```

## üöÄ GitHub Actions Workflow Configuration

To enable full automation, add the following GitHub Secrets:
| Secret Name | Description |
|-----|-----------|
| **GCP_SA_KEY** | Entire contents of bigquery-key.json |

The workflow automatically:

- Installs Playwright browser binaries
- Sets up Python dependencies
- Authenticates with BigQuery
- Executes the ETL pipeline on schedule

## What This Project Demonstrates

- Real-world ETL pipeline design
- Cloud-native ELT best practices
- Idempotent data engineering logic
- CI/CD-driven automation
- Analytics-ready data modeling
	
## Future Enhancements

- Add data quality checks (Great Expectations)
- Implement incremental scraping logic
- Add alerting on pipeline failures
- Extend to additional startup ecosystems

## Introduction

## Project Overview

## Tech Stack

## Architecture Diagram

## Dataset Used

## Data Model used
